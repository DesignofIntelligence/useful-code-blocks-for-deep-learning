{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-6da1150694b8>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m##USE HISTORY TO PLOT THE TRAINING\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "##USE HISTORY TO PLOT THE TRAINING\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "history = model.fit()\n",
    "\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##EVALUATE THE MODEL\n",
    "model.evaluate(X_test, y_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##STOP TRAINING\n",
    "#tf.earlystopping_callback\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##PLOT A HISTOGRAM\n",
    "\n",
    "X[\"age\"].plot(kind=\"hist\") #histogram\n",
    "X[\"age\"].value_counts()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##DATA SCALING NORMALIZATION OR STANDARDIZATION\n",
    "normalization: make values between 0 and 1, minmax normalization(scaler)\n",
    "standardization removes teh mean and divides by standard deviation\n",
    "\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##ONE HOT ENCODE\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ct = make_column_transformer((MinMaxScaler(),['column','column']),\n",
    "\t\t\t     (OneHotEncoder(handle_unknown=\"ignore\"), ['column']))\n",
    "\n",
    "#or\n",
    "tf.one_hot(list,depth=class_number) #use categorical instead of sparsecategorical\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## SPLIT DATA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size =0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## CREATE RANDOM DATA\n",
    "from sklearn.datasets import make_circles\n",
    "n_samples = 1000\n",
    "X, y = make_circles(n_samples, noise = 0.03, random_state = 42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualize a dataset of X1,X2 coordinates and a class (gadwal w plot)\n",
    "circles = pd.DataFrame({\"X0:X[:,0], :\"X1\":X[:,1], \"label\":y}\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu);\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#visualize the decision boundary\n",
    "import numpy as np\n",
    "def plot_decision_boundary(model, X, y):\n",
    "\tx_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "\ty_min, y_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "\txx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "\t\t\t     np.linespace(y_min, y_max, 100))\n",
    "\n",
    "\tx_in = np.c_[xx.ravel(), yy.ravel()]\n",
    "\ty_pred = model.predict(x_in)\n",
    "\tif len(y_pred[0]) > 1:\n",
    "\t\ty_pred = np.argmax(y_pred,axis = 1).reshape(xx.shape)\n",
    "\telse:\n",
    "\t\ty_pred = np.round(y_pred).reshape(xx.shape)\n",
    "\tplt.contourf(xx,yy, y_pred, cmap = plt.cm.RdYlBu, alpha = 0.7)\n",
    "\tplt.scatter(X[:,0], X[:,1], c=y, s=40, cmap=plt.cm.RdYlBu)\n",
    "\tplt.xlim(xx.min(), xx.max())\n",
    "\tplt.ylimt(yy.min(),yy.max())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## LR SCHEDULER\n",
    "lr_schedular = tf.keras.callbacks.LearningRateScheduler(lambda epoch = 1e-4 * 10 **(epoch/20))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#EQ like graph\n",
    "plt.semilogx()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_preds = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_preds)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#prettify a confusion matrix\n",
    "import itertools\n",
    "\n",
    "figsize = (10,10)\n",
    "\n",
    "cm = confusion_matrix(y_test, tf.round(y_preds))\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:,np.newaxis]\n",
    "n_classes = cm.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = figsize)\n",
    "cax = ax.matshow(cm,cmap=plt.cm.Blues)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "classes = False\n",
    "\n",
    "if classes:\n",
    "\tlabels = classes\n",
    "else:\n",
    "\tlabels = np.arange(cm.shape[0])\n",
    "\n",
    "ax.set(title=\"Confusion Matrix\",\n",
    "\txlabel=\"predicted\",\n",
    "\tylabel=\"true\",\n",
    "\txticks=np.arange(n_classes),\n",
    "\tytikcs=np.arange(n_classes),\n",
    "\txticklabels=labels,\n",
    "\tyticklabels=labels)\n",
    "\n",
    "threshold = (cm.max()+cm.min()) /2.\n",
    "\n",
    "for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "\tplt.text(j, i, f\"{cm[i, j]} ({cm_norm[i,j]*100:.1f}%)\", horizontalalignment =\"center\",\n",
    "\tcolor =\"white\" if cm[i,j] > threshold else \"black\", size = 15)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-0416cd77f164>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#plot models\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mplot_model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mplot_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mshow_shapes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#plot models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-45ebdb9cf69d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m## SCALE IMAGE BETWEEN 0 AND 1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimage\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mImageDataGenerator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mtrain_datagen\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mImageDataGenerator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrescale\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1.\u001B[0m\u001B[1;33m/\u001B[0m\u001B[1;36m255\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m train_datagen.flow_from_directory(directory=train_dir,\n\u001B[0;32m      5\u001B[0m                                                                   \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "## SCALE IMAGE BETWEEN 0 AND 1\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_data = train_datagen.flow_from_directory(directory=train_dir,\n",
    "\t\t\t\t\t\t\t\t  batch_size=32,\n",
    "\t\t\t\t\t\t\t\t  target_size=(224,224),\n",
    "\t\t\t\t\t\t\t\t  class_mode=\"binary\",\n",
    "\t\t\t\t\t\t\t\t  seed=42)\n",
    "#THERE IS ALSO tf.keras.preprocessing.image_dataset_from_directory()\n",
    "# note that you can only pass the train_data directly without labels to model.fit\n",
    "# example:\n",
    "history_1 = model.fit(train_data,\n",
    "\t\t\t\t\t  epochs=5,\n",
    "\t\t\t\t\t  steps_per_epoch=len(train_data),\n",
    "\t\t\t\t\t  validation_data=valid_data,\n",
    "\t\t\t\t\t  validation_steps=len(valid_data))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##DATA AUGMENTATION\n",
    "train_data_augemented = ImageDataGenerator(rescale=1/255.,\n",
    "\t\t\t\t\t\t\t\t\t\t   rotation_range=0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t   shear_range=0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t   zoom_range=0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t   width_shift_range=0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t   height_shift_range=0.2,\n",
    "\t\t\t\t\t\t\t\t\t\t   horizontal_flip=True)\n",
    "\n",
    "##ALTERNATIVE, useful because exploits gpu power, also useful because its a part of the model now, u dont need to processes it on data everytime\n",
    "train_data_augemented2 = keras.Sequential([\n",
    "\ttf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "\ttf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "\ttf.keras.layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "\ttf.keras.layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "\ttf.keras.layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "\ttf.keras.layers.experimental.preprocessing.Rescale(1./255)\n",
    "\t])\n",
    "train_data_augemented2(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget link  #downloads a file to google collab, can be from github\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##CHANGE LABEL CLASS TO NUMBERS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "train_labels['code'] = LE.fit_transform(train_labels['Class'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#one hot encode a class label and change it to 2D list encode\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "list_of_labels = list(train_labels[\"Class\"])\n",
    "one_hot_data=pd.get_dummies(train_labels, prefix=\"\")\n",
    "one_hot_data = one_hot_data.drop(columns=\"ID\")\n",
    "y = LabelBinarizer().fit_transform(one_hot_data)\n",
    "one_hot_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#SAVE A MODEL\n",
    "model_11.save(\"saved_trained_model\")\n",
    "loaded_model_11 = tf.keras.models.load_model(\"saved_trained_model\")\n",
    "loaded_model_11.evaluate(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir\n",
    "  )\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback\n",
    "\n",
    "#THEN ADD THIS TO .fit\n",
    "callbacks = [create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
    "\t\t\t\t\t\t\t\t\t\t experiment_name=\"experiment_name\")]\n",
    "\n",
    "\"\"\"\n",
    "$ pip install -U tensorboard\n",
    "\n",
    "# For help, run \"tensorboard dev --help\" or \"tensorboard dev COMMAND --help\"\n",
    "\n",
    "# Upload an experiment:\n",
    "$ tensorboard dev upload --logdir logs \\\n",
    "    --name \"(optional) My latest experiment\" \\\n",
    "    --description \"(optional) Simple comparison of several hyperparameters\"\n",
    "\n",
    "***** TensorBoard Uploader *****\n",
    "\n",
    "This TensorBoard will be visible to everyone. Do not upload sensitive data.\n",
    "\n",
    "Continue? (yes/NO)\n",
    "\n",
    "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth...\n",
    "\n",
    "Uploading to TensorBoard.dev at https://tensorboard.dev/experiment/QFRIzZJpTZCNRzi8N7zomA\n",
    "\"\"\"\n",
    "!tensorboard dev upload --logdir ./tensorflow_hub/ \\\n",
    "\t--name \"name\" \\\n",
    "\t--descrption \"something\" \\\n",
    "\t--one_shot\n",
    "\n",
    "!tensorboard dev list #display the list of experiments\n",
    "\n",
    "!tensorboard dev delete --experiment_id SSJAJ38UD83"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## TRANSFER LEARNING: FEATURE EXTRACTION\n",
    "\n",
    "def create_model(model_url, num_classes=10):\n",
    "\n",
    "  feature_extractor_layer = hub.KerasLayer(model_url,\n",
    "                                           trainable=False, # freeze the underlying patterns\n",
    "                                           name='feature_extraction_layer',\n",
    "                                           input_shape=IMAGE_SHAPE+(3,)) # define the input image shape\n",
    "  model = tf.keras.Sequential([\n",
    "    feature_extractor_layer, # use the feature extraction layer as the base\n",
    "    layers.Dense(num_classes, activation='softmax', name='output_layer') # create our own output layer      \n",
    "  ])\n",
    "\n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##FUNCTIONAL API (instead of sequential) + FEATURE EXTRACTION + MODEL INSIDE TF\n",
    "\n",
    "model = tf.keras.applications.EfficientNetB0(include_top=False) #dont include the output layer\n",
    "\n",
    "##FREEZE THE MODEL\n",
    "model.trainable = False\n",
    "\n",
    "#create inputs\n",
    "inputs = tf.keras.layers.Input(Shape=(224,224,3), name =\"input_layer\")\n",
    "\n",
    "#normalize NOTE THAT RESNET NEEDS NORMALIZATION WHILE EFFICIENTB0 DOESNT NEED because it has a rescaling layer\n",
    "#x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
    "\n",
    "#pass inputs\n",
    "x = model(inputs)\n",
    "\n",
    "# AVERAGE POOL\n",
    "x = tf.keras.layers.GlobalAveragePooling2d(name=\"global_average_pool\")(x)\n",
    "print(x.shape)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(10, activation = \"softmax\", name=\"output_layer\")(x)\n",
    "\n",
    "model_0 = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "##MODEL FIT\n",
    "history = model_0.fit(train_data,\n",
    "\t\t\t\t\t  epochs=5,\n",
    "\t\t\t\t\t  steps_per_epoch= len(train_data),\n",
    "\t\t\t\t\t  validation_data=test_data,\n",
    "\t\t\t\t\t  validation_steps=int(0.25* len(test_data)),\n",
    "\t\t\t\t\t  callbacks=[create_tensorboard_callback(dir_name=\"tensorboard/\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t experiment_name=\"something\")]) #Only relevant if validation_data is provided and is a tf.data dataset. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.\n",
    "\n",
    "##CHECK THE LAYERS THAT WE HAVE\n",
    "for layer_number, layer in enumerate(model.layers):\n",
    "\tprint(layer_number, layer.name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##GLOBAL AVERAGE POOLING\n",
    "takes (1,4,4,3) and changes it into (1,3) ## kills inner two dimensions by getting their averages, aka gets average of every feature map and changes it into one element, so that u have a feature vector from all the feature maps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## CREATE A MODEL CHECKPOINT\n",
    "checkpoint_path = \"something/\"\n",
    "checkpoint_callback =tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "\t\t\t\t\t\t\t\t   \t\t\t\t\t\tsave_weights_only=True,\n",
    "\t\t\t\t\t\t\t\t   \t\t\t\t\t\tsave_best_only=False, #SAVES MODEL WITH LOWEST VALIDATION LOSS\n",
    "\t\t\t\t\t\t\t\t   \t\t\t\t\t\tsave_freq=True) #saves every epoch\n",
    "\n",
    "##LOAD THE MODEL\n",
    "model.load_weights(checkpoint_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CHECK IF TWO NUMBERS ARE CLOSE TO EACH OTHER\n",
    "np.isclose(np.array(),np.array())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TRANSFER LEARNING: FINE TUNING\n",
    "## first create feature extraction model, train the output weights,then train layers inside\n",
    "model.trainable=True\n",
    "for layer in model.layers[:-10]:\n",
    "\tlayer.trainable=False\n",
    "## then recompile, with a lower learning rate to prevent overfitting\n",
    "\n",
    "#GET NUMBER OF TRAINABLE VARIABLES\n",
    "len(model.trainable_variables)\n",
    "\n",
    "#start training from last epoch:\n",
    "model.fit(initial_epoch=history.epoch[-1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}